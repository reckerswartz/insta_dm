# Local AI Services Configuration
# Set these environment variables to configure local AI services

# Local AI Microservice (Python)
# The microservice provides: YOLOv8, RetinaFace, InsightFace, PaddleOCR, Whisper
LOCAL_AI_SERVICE_URL=http://localhost:8000
USE_LOCAL_AI_MICROSERVICE=true
# When false, overall local AI health only requires Ollama to be healthy.
LOCAL_AI_MICROSERVICE_REQUIRED=false
LOCAL_AI_ENABLE_VISION=true
LOCAL_AI_ENABLE_VIDEO=true
LOCAL_AI_ENABLE_FACE=true
LOCAL_AI_ENABLE_OCR=true
LOCAL_AI_ENABLE_WHISPER=true
# If default startup fails, local_ai_services retries with face/ocr/whisper disabled.
MICROSERVICE_SAFE_MODE_FALLBACK=true
# Force safe mode startup every time (disables face/ocr/whisper services).
MICROSERVICE_FORCE_SAFE_MODE=false

# Ollama (Local LLM)
# Provides local LLM inference for vision understanding + comment generation
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2-vision:11b
OLLAMA_FAST_MODEL=llama3.2-vision:11b
OLLAMA_QUALITY_MODEL=llama3.2-vision:11b
OLLAMA_VISION_MODEL=llama3.2-vision:11b
OLLAMA_COMMENT_MODEL=llama3.2-vision:11b
OLLAMA_VISION_UNDERSTANDING_ENABLED=true
OLLAMA_NUM_CTX=3072
# Optional CPU thread cap (leave unset to let Ollama decide)
# OLLAMA_NUM_THREAD=8

# LLM Comment Routing/Tuning
LLM_COMMENT_ENABLE_MODEL_ESCALATION=true
LLM_COMMENT_ESCALATION_MIN_ACCEPTED=5
LLM_COMMENT_ESCALATION_MAX_REJECT_RATIO=0.45
LLM_COMMENT_ESCALATION_MIN_GROUNDED_RATIO=0.55
LLM_COMMENT_MAX_CONTEXT_JSON_CHARS=3200
LLM_COMMENT_TARGET_CONTEXT_JSON_CHARS=2600
LLM_COMMENT_PRIMARY_MAX_TOKENS=180
LLM_COMMENT_PRIMARY_RETRY_MAX_TOKENS=130
LLM_COMMENT_QUALITY_MAX_TOKENS=240
LLM_COMMENT_QUALITY_RETRY_MAX_TOKENS=170

# Post comment suitability + relevance gates
POST_COMMENT_MIN_PERSONAL_SIGNAL_SCORE=2
POST_COMMENT_MIN_RELEVANCE_SCORE=1.1
POST_COMMENT_MIN_ELIGIBLE_SUGGESTIONS=2
POST_COMMENT_HIGH_RELEVANCE_OVERRIDE_SCORE=1.55

# Whisper Configuration (Fallback)
# Used if microservice is unavailable
WHISPER_BIN=whisper
WHISPER_MODEL=base

# Face Recognition Settings
FACE_RECOGNITION_THRESHOLD=0.6
FACE_EMBEDDING_SIZE=512

# Video Processing Settings
VIDEO_SAMPLE_RATE=2  # Sample every 2 seconds
MAX_VIDEO_SIZE_MB=100

# Performance Settings
AI_REQUEST_TIMEOUT=120
AI_CONCURRENT_REQUESTS=3
AI_VISUAL_TIMEOUT_SECONDS=210
AI_FACE_TIMEOUT_SECONDS=180
AI_OCR_TIMEOUT_SECONDS=150
AI_VIDEO_TIMEOUT_SECONDS=180

# AI Queue Concurrency (Sidekiq capsules)
SIDEKIQ_AI_CONCURRENCY=1
SIDEKIQ_AI_PROFILE_ANALYSIS_CONCURRENCY=1
SIDEKIQ_AI_POST_ANALYSIS_CONCURRENCY=2
SIDEKIQ_AI_PROFILE_HISTORY_CONCURRENCY=1
SIDEKIQ_AI_LLM_COMMENT_CONCURRENCY=2
SIDEKIQ_AI_COMMENT_GENERATION_CONCURRENCY=1
SIDEKIQ_AI_PIPELINE_ORCHESTRATION_CONCURRENCY=1
SIDEKIQ_AI_PROFILE_IMAGE_DESCRIPTION_CONCURRENCY=1
SIDEKIQ_AI_VISUAL_CONCURRENCY=2
SIDEKIQ_AI_FACE_CONCURRENCY=2
SIDEKIQ_AI_FACE_REFRESH_CONCURRENCY=1
SIDEKIQ_AI_OCR_CONCURRENCY=1
SIDEKIQ_AI_VIDEO_CONCURRENCY=1
SIDEKIQ_AI_METADATA_CONCURRENCY=1
SIDEKIQ_STORY_ANALYSIS_CONCURRENCY=1

# Face refresh dedupe/backlog protection
PROFILE_HISTORY_FACE_REFRESH_PENDING_WINDOW_HOURS=72
PROFILE_HISTORY_FACE_REFRESH_DUPLICATE_SKIP_DAYS=7

# Resource Guard (defer non-critical OCR/video work under pressure)
AI_MAX_LOAD_PER_CORE=1.20
AI_MIN_AVAILABLE_MEMORY_MB=700
AI_MAX_QUEUE_DEPTH=220
AI_RESOURCE_RETRY_SECONDS=20

# Health Cache
AI_HEALTH_CACHE_TTL_SECONDS=900
AI_HEALTH_STALE_AFTER_SECONDS=240

# Cost Optimization
ENABLE_LOCAL_FALLBACK=true
PREFER_LOCAL_OVER_CLOUD=true

# Local AI Services Configuration
# Set these environment variables to configure local AI services

# Local AI Microservice (Python)
# The microservice provides: YOLOv8, RetinaFace, InsightFace, PaddleOCR, Whisper
LOCAL_AI_SERVICE_URL=http://localhost:8000
# Keep disabled by default for lightweight local runs; enable only when face/OCR microservice paths are needed.
USE_LOCAL_AI_MICROSERVICE=false
# When false, overall local AI health only requires Ollama to be healthy.
LOCAL_AI_MICROSERVICE_REQUIRED=false
# Fail fast on local microservice network stalls and cooldown repeated failures.
LOCAL_AI_HTTP_OPEN_TIMEOUT_SECONDS=4
LOCAL_AI_HTTP_READ_TIMEOUT_SECONDS=45
LOCAL_AI_UNAVAILABLE_COOLDOWN_SECONDS=90
LOCAL_AI_ENABLE_VISION=true
LOCAL_AI_ENABLE_VIDEO=true
LOCAL_AI_ENABLE_FACE=true
LOCAL_AI_ENABLE_OCR=true
LOCAL_AI_ENABLE_WHISPER=true
# If default startup fails, local_ai_services retries with face/ocr/whisper disabled.
MICROSERVICE_SAFE_MODE_FALLBACK=true
# Force safe mode startup every time (disables face/ocr/whisper services).
MICROSERVICE_FORCE_SAFE_MODE=false

# Ollama (Local LLM)
# Provides local LLM inference for vision understanding + comment generation
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_FAST_MODEL=llama3.2:3b
OLLAMA_QUALITY_MODEL=llama3.2:3b
OLLAMA_COMMENT_QUALITY_MODEL=llama3.2:3b
OLLAMA_VISION_MODEL=llava:7b
OLLAMA_COMMENT_MODEL=llama3.2:3b
OLLAMA_VISION_UNDERSTANDING_ENABLED=true
OLLAMA_VISION_MAX_IMAGES=2
OLLAMA_NUM_CTX=2048
# Optional CPU thread cap (leave unset to let Ollama decide)
# OLLAMA_NUM_THREAD=8

# Lightweight provider pre-analysis (skip LLM vision when local signals are sufficient)
LOCAL_PROVIDER_LIGHTWEIGHT_MODE=true
LOCAL_PROVIDER_MIN_LOCAL_SIGNALS_FOR_VISION_SKIP=3
LOCAL_PROVIDER_DYNAMIC_KEYFRAME_LIMIT=2

# LLM Comment Routing/Tuning
LLM_COMMENT_ENABLE_MODEL_ESCALATION=false
LLM_COMMENT_ESCALATION_MIN_ACCEPTED=5
LLM_COMMENT_ESCALATION_MAX_REJECT_RATIO=0.45
LLM_COMMENT_ESCALATION_MIN_GROUNDED_RATIO=0.55
LLM_COMMENT_MAX_CONTEXT_JSON_CHARS=1800
LLM_COMMENT_TARGET_CONTEXT_JSON_CHARS=1300
LLM_COMMENT_PRIMARY_MAX_TOKENS=110
LLM_COMMENT_PRIMARY_RETRY_MAX_TOKENS=90
LLM_COMMENT_QUALITY_MAX_TOKENS=150
LLM_COMMENT_QUALITY_RETRY_MAX_TOKENS=110

# Post comment suitability + relevance gates
POST_COMMENT_MIN_PERSONAL_SIGNAL_SCORE=2
POST_COMMENT_MIN_RELEVANCE_SCORE=1.1
POST_COMMENT_MIN_ELIGIBLE_SUGGESTIONS=2
POST_COMMENT_HIGH_RELEVANCE_OVERRIDE_SCORE=1.55

# Whisper Configuration (Fallback)
# Used if microservice is unavailable
WHISPER_BIN=whisper
WHISPER_MODEL=tiny
LOCAL_AI_TRANSCRIBE_OPEN_TIMEOUT_SECONDS=3
LOCAL_AI_TRANSCRIBE_READ_TIMEOUT_SECONDS=25
LOCAL_AI_TRANSCRIBE_FAILURE_COOLDOWN_SECONDS=120

# Face Recognition Settings
FACE_RECOGNITION_THRESHOLD=0.6
FACE_EMBEDDING_SIZE=512

# Video Processing Settings
VIDEO_SAMPLE_RATE=3  # Sample every 3 seconds
MAX_VIDEO_SIZE_MB=100
POST_VIDEO_LIGHTWEIGHT_MODE=true
POST_VIDEO_VISION_FRAME_SAMPLE_LIMIT=2
POST_VIDEO_DYNAMIC_KEYFRAME_LIMIT=2
POST_VIDEO_DYNAMIC_FRAME_INTERVAL_SECONDS=5.0
POST_VIDEO_AUDIO_PRIORITY_MIN_WORDS=8
POST_VIDEO_MIN_STRUCTURED_SIGNALS_FOR_SKIP=2
POST_VIDEO_SKIP_DYNAMIC_VISION_WHEN_AUDIO_PRESENT=true
POST_VIDEO_EXTRACTION_PROFILE=lightweight_v1

# Python video microservice profile
LOCAL_AI_VIDEO_SAMPLE_RATE_SECONDS=3
LOCAL_AI_VIDEO_MAX_FRAMES=12
LOCAL_AI_VIDEO_RESIZE_MAX_WIDTH=960
LOCAL_AI_VIDEO_STATIC_PREFILTER=true
LOCAL_AI_VIDEO_FRAME_CACHE_SIZE=64

# OCR/Whisper model controls
LOCAL_AI_ENABLE_PADDLE_OCR=true
LOCAL_AI_ENABLE_EASY_OCR=false
LOCAL_WHISPER_MODEL=tiny
LOCAL_WHISPER_COMPUTE_TYPE=int8
LOCAL_WHISPER_ALLOW_DYNAMIC_MODEL_LOADING=false

# Performance Settings
AI_REQUEST_TIMEOUT=120
AI_CONCURRENT_REQUESTS=2
AI_VISUAL_TIMEOUT_SECONDS=210
AI_FACE_TIMEOUT_SECONDS=180
AI_OCR_TIMEOUT_SECONDS=150
AI_VIDEO_TIMEOUT_SECONDS=120
AI_VIDEO_FAST_FAIL_ON_TIMEOUT=true
AI_PIPELINE_VIDEO_REINITIALIZE_ATTEMPTS=0
AI_PIPELINE_VIDEO_FAILURE_FALLBACK_ENABLED=true
AI_PIPELINE_VIDEO_FAILURE_FALLBACK_REQUIRE_VISUAL_SUCCESS=true

# AI Queue Concurrency (Sidekiq capsules)
# Compact mode (recommended locally): run AI + story/background lanes with fewer shared capsules.
SIDEKIQ_COMPACT_CAPSULES=true
SIDEKIQ_AI_COMPACT_CONCURRENCY=2
SIDEKIQ_BACKGROUND_COMPACT_CONCURRENCY=2
# Set SIDEKIQ_COMPACT_CAPSULES=false to use the dedicated per-queue capsule settings below.
SIDEKIQ_AI_PROFILE_ANALYSIS_CONCURRENCY=1
SIDEKIQ_AI_POST_ANALYSIS_CONCURRENCY=1
SIDEKIQ_AI_PROFILE_HISTORY_CONCURRENCY=1
SIDEKIQ_AI_LLM_COMMENT_CONCURRENCY=1
SIDEKIQ_AI_COMMENT_GENERATION_CONCURRENCY=1
SIDEKIQ_AI_PIPELINE_ORCHESTRATION_CONCURRENCY=1
SIDEKIQ_AI_PROFILE_IMAGE_DESCRIPTION_CONCURRENCY=1
SIDEKIQ_AI_VISUAL_CONCURRENCY=1
SIDEKIQ_AI_FACE_CONCURRENCY=1
SIDEKIQ_AI_FACE_REFRESH_CONCURRENCY=1
SIDEKIQ_AI_OCR_CONCURRENCY=1
SIDEKIQ_AI_VIDEO_CONCURRENCY=1
SIDEKIQ_AI_METADATA_CONCURRENCY=1
SIDEKIQ_STORY_ANALYSIS_CONCURRENCY=1

# Face refresh dedupe/backlog protection
PROFILE_HISTORY_FACE_REFRESH_PENDING_WINDOW_HOURS=72
PROFILE_HISTORY_FACE_REFRESH_DUPLICATE_SKIP_DAYS=7

# Resource Guard (defer non-critical OCR/video work under pressure)
AI_MAX_LOAD_PER_CORE=1.20
AI_MIN_AVAILABLE_MEMORY_MB=700
AI_MAX_QUEUE_DEPTH=220
AI_RESOURCE_RETRY_SECONDS=20

# Health Cache
AI_HEALTH_CACHE_TTL_SECONDS=900
AI_HEALTH_STALE_AFTER_SECONDS=240

# Queue timing metrics + ETA forecasting
JOB_EXECUTION_METRICS_ENABLED=true
JOB_EXECUTION_METRICS_WINDOW_HOURS=24
JOB_EXECUTION_METRICS_CACHE_TTL_SECONDS=20
QUEUE_ESTIMATOR_LOOKBACK_HOURS=24
QUEUE_ESTIMATOR_CACHE_TTL_SECONDS=15
LLM_COMMENT_DEFAULT_QUEUE_ITEM_SECONDS=120

# Cost Optimization
ENABLE_LOCAL_FALLBACK=true
PREFER_LOCAL_OVER_CLOUD=true

# Local AI Stack - Quick Installation Guide

## ðŸš€ One-Command Installation

Run this single command to install and configure the entire local AI stack:

```bash
./install_local_ai_stack.sh
```

## ðŸ“‹ What This Script Does

### **System Setup**
- âœ… Checks Linux system compatibility
- âœ… Installs system dependencies (Python 3.12, build tools, libraries)
- âœ… Installs AI/ML libraries (OpenCV, FFmpeg, etc.)
- âœ… Verifies hardware requirements (RAM, disk space)

### **AI Services Installation**
- âœ… Installs Ollama (LLM service)
- âœ… Pulls Mistral 7B model
- âœ… Sets up Python virtual environment
- âœ… Installs all Python packages (FastAPI, YOLOv8, etc.)

### **Rails Integration**
- âœ… Installs Rails dependencies
- âœ… Runs database migrations
- âœ… Configures local AI provider as default
- âœ… Disables cloud providers

### **Configuration & Verification**
- âœ… Creates necessary directories
- âœ… Sets up environment variables
- âœ… Starts all services
- âœ… Runs comprehensive tests
- âœ… Verifies full pipeline

## ðŸŽ¯ After Installation

### **Services Running**
- **AI Microservice**: http://localhost:8000
- **Ollama LLM**: http://localhost:11434
- **Rails App**: http://localhost:3000 (when started)

### **Quick Commands**
```bash
# Start everything
./bin/dev

# Check service status
./bin/local_ai_services status

# Run diagnostics
./bin/diagnose_ai

# Validate setup
./bin/validate_local_ai
```

## ðŸ“Š System Requirements

### **Minimum Requirements**
- **OS**: Linux (Ubuntu 20.04+)
- **RAM**: 16GB (8GB minimum)
- **Storage**: 5GB free
- **Architecture**: x86_64

### **Recommended Requirements**
- **RAM**: 32GB+ (for optimal performance)
- **CPU**: Multi-core processor
- **Storage**: 10GB+ free

## ðŸ”§ Customization

### **Environment Variables**
Edit `.env` file after installation:

```bash
# Change LLM model
OLLAMA_MODEL=llama3:8b

# Adjust performance
AI_REQUEST_TIMEOUT=120
VIDEO_SAMPLE_RATE=2

# Service URLs
LOCAL_AI_SERVICE_URL=http://localhost:8000
OLLAMA_URL=http://localhost:11434
```

### **Model Options**
```bash
# Available models (run after installation)
ollama list

# Pull additional models
ollama pull llama3:8b
ollama pull phi3:mini
```

## ðŸš¨ Troubleshooting

### **Common Issues**

**Installation fails:**
```bash
# Check logs
cat installation.log

# Re-run with verbose output
bash -x ./install_local_ai_stack.sh
```

**Services won't start:**
```bash
# Check status
./bin/local_ai_services status

# View logs
./bin/local_ai_services logs

# Restart services
./bin/local_ai_services restart
```

**Performance issues:**
```bash
# Check system resources
htop
free -h
df -h

# Reduce concurrent requests
export AI_CONCURRENT_REQUESTS=1
```

**Model issues:**
```bash
# Re-download model
ollama pull mistral:7b

# Check available models
ollama list
```

## ðŸ“ˆ Performance Expectations

### **Processing Times**
- **Image Analysis**: 2-5 seconds
- **Face Recognition**: 1-3 seconds
- **OCR**: 2-4 seconds
- **Comment Generation**: 10-30 seconds
- **Video Analysis**: 30-120 seconds

### **Cost Impact**
- **Cloud AI Costs**: $0.00 (100% savings)
- **Local Processing**: Free (after initial setup)
- **Hardware Usage**: CPU/RAM intensive during processing

## ðŸ”„ Updates & Maintenance

### **Update Models**
```bash
# Update Ollama
ollama pull mistral:7b

# Update Python packages
cd ai_microservice
source ai_microservice_env/bin/activate
pip install --upgrade -r requirements.txt
```

### **Backup Configuration**
```bash
# Backup environment
cp .env .env.backup

# Backup logs
tar -czf logs_backup.tar.gz log/
```

## ðŸŽ‰ Success Criteria

After running the script, you should have:

âœ… **All services running** without errors  
âœ… **Full AI pipeline** working (image â†’ analysis â†’ comments)  
âœ… **100% cost reduction** compared to cloud services  
âœ… **Rails integration** seamless and functional  
âœ… **Performance** acceptable for your hardware  

## ðŸ†˜ Support

If you encounter issues:

1. **Check the installation log**: `cat installation.log`
2. **Run diagnostics**: `./bin/diagnose_ai`
3. **Check service logs**: `./bin/local_ai_services logs`
4. **Verify setup**: `./bin/validate_local_ai`

---

**ðŸš€ Ready to automate your entire local AI stack setup! Run `./install_local_ai_stack.sh` to get started.**

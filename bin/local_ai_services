#!/bin/bash

# Local AI Services Management Script

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color
HEALTHCHECK_TIMEOUT_SECONDS="${HEALTHCHECK_TIMEOUT_SECONDS:-5}"
MICROSERVICE_STARTUP_TIMEOUT_SECONDS="${MICROSERVICE_STARTUP_TIMEOUT_SECONDS:-90}"
MICROSERVICE_STABILITY_WINDOW_SECONDS="${MICROSERVICE_STABILITY_WINDOW_SECONDS:-18}"
MICROSERVICE_SAFE_MODE_FALLBACK="${MICROSERVICE_SAFE_MODE_FALLBACK:-true}"
MICROSERVICE_FORCE_SAFE_MODE="${MICROSERVICE_FORCE_SAFE_MODE:-false}"
OLLAMA_VISION_MODEL="${OLLAMA_VISION_MODEL:-llama3.2-vision:11b}"
OLLAMA_BASE_MODEL="${OLLAMA_MODEL:-llama3.2:3b}"
OLLAMA_COMMENT_MODEL="${OLLAMA_COMMENT_MODEL:-$OLLAMA_BASE_MODEL}"
OLLAMA_PRIMARY_MODEL="${OLLAMA_FAST_MODEL:-${OLLAMA_COMMENT_MODEL:-$OLLAMA_BASE_MODEL}}"
OLLAMA_QUALITY_MODEL="${OLLAMA_QUALITY_MODEL:-$OLLAMA_PRIMARY_MODEL}"

# Function to print colored output
print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_header() {
    echo -e "${BLUE}=== $1 ===${NC}"
}

is_truthy() {
    local value="${1:-}"
    value="$(printf '%s' "$value" | tr '[:upper:]' '[:lower:]')"
    [[ "$value" == "1" || "$value" == "true" || "$value" == "yes" || "$value" == "on" ]]
}

microservice_enabled() {
    is_truthy "${USE_LOCAL_AI_MICROSERVICE:-true}"
}

configured_ollama_models() {
    printf "%s\n" \
        "$OLLAMA_BASE_MODEL" \
        "$OLLAMA_PRIMARY_MODEL" \
        "$OLLAMA_QUALITY_MODEL" \
        "$OLLAMA_COMMENT_MODEL" \
        "$OLLAMA_VISION_MODEL" \
        "${OLLAMA_COMMENT_QUALITY_MODEL:-}" \
        | sed '/^[[:space:]]*$/d' \
        | sort -u
}

model_in_list() {
    local needle="$1"
    shift || true
    local item
    for item in "$@"; do
        if [[ "$item" == "$needle" ]]; then
            return 0
        fi
    done
    return 1
}

ensure_ollama_model() {
    local model_name="$1"
    [ -n "$model_name" ] || return 0

    if ollama list 2>/dev/null | awk 'NR>1 {print $1}' | grep -Fxq "$model_name"; then
        return 0
    fi

    print_status "Pulling model $model_name (this may take a while)..."
    ollama pull "$model_name"
}

port_listener_pids() {
    local port=$1
    ss -ltnp "( sport = :$port )" 2>/dev/null \
        | sed -n 's/.*pid=\([0-9][0-9]*\).*/\1/p' \
        | sort -u
}

is_microservice_process() {
    local pid=$1
    local cmd

    cmd=$(ps -p "$pid" -o args= 2>/dev/null || true)
    [ -n "$cmd" ] && [[ "$cmd" == *"ai_microservice/main.py"* ]]
}

stop_stale_microservice_listener() {
    local had_stale=0
    local pid

    while IFS= read -r pid; do
        [ -n "$pid" ] || continue
        if ! is_microservice_process "$pid"; then
            continue
        fi

        had_stale=1
        print_warning "Stopping stale AI Microservice process on port 8000 (PID: $pid)"
        kill "$pid" 2>/dev/null || true

        for _ in $(seq 1 20); do
            if ! kill -0 "$pid" 2>/dev/null; then
                break
            fi
            sleep 0.2
        done

        if kill -0 "$pid" 2>/dev/null; then
            print_warning "Force killing stale AI Microservice process (PID: $pid)"
            kill -9 "$pid" 2>/dev/null || true
        fi
    done < <(port_listener_pids 8000)

    if [ "$had_stale" -eq 1 ]; then
        rm -f "$PROJECT_ROOT/tmp/ai_microservice.pid"
        sleep 1
    fi
}

# Check if service is running
check_service() {
    local service_name=$1
    local port=$2
    local path=${3:-/health}
    local expected_pattern=${4:-}
    local url="http://localhost:$port$path"
    local response

    if ! response=$(curl -sS --max-time "$HEALTHCHECK_TIMEOUT_SECONDS" "$url" 2>/dev/null); then
        print_warning "$service_name is not responding on port $port"
        return 1
    fi

    if [ -n "$expected_pattern" ] && ! printf '%s' "$response" | grep -Eq "$expected_pattern"; then
        print_warning "$service_name responded on port $port but did not pass health validation"
        return 1
    fi

    print_status "$service_name is running on port $port"
    return 0
}

wait_for_service_ready() {
    local service_name=$1
    local port=$2
    local path=${3:-/health}
    local expected_pattern=${4:-}
    local timeout_seconds=${5:-60}
    local started_at=$SECONDS

    while (( SECONDS - started_at < timeout_seconds )); do
        if check_service "$service_name" "$port" "$path" "$expected_pattern" >/dev/null 2>&1; then
            return 0
        fi
        sleep 2
    done

    check_service "$service_name" "$port" "$path" "$expected_pattern"
    return 1
}

ensure_service_stable() {
    local service_name=$1
    local port=$2
    local path=${3:-/health}
    local expected_pattern=${4:-}
    local stability_window=${5:-15}
    local started_at=$SECONDS

    while (( SECONDS - started_at < stability_window )); do
        if ! check_service "$service_name" "$port" "$path" "$expected_pattern" >/dev/null 2>&1; then
            print_warning "$service_name became unhealthy during stability window"
            return 1
        fi
        sleep 2
    done

    return 0
}

start_microservice_process() {
    local mode_label=$1
    shift
    local -a extra_env=("$@")

    print_status "Starting AI Microservice (${mode_label})..."
    cd "$PROJECT_ROOT"

    if [ "${#extra_env[@]}" -gt 0 ]; then
        env PYTHONPATH="ai_microservice" "${extra_env[@]}" \
            nohup ai_microservice/ai_microservice_env/bin/python ai_microservice/main.py > log/ai_microservice.log 2>&1 &
    else
        PYTHONPATH="ai_microservice" \
            nohup ai_microservice/ai_microservice_env/bin/python ai_microservice/main.py > log/ai_microservice.log 2>&1 &
    fi
    MICROSERVICE_PID=$!
    echo $MICROSERVICE_PID > tmp/ai_microservice.pid
}

# Start Ollama
start_ollama() {
    print_header "Starting Ollama"
    
    if check_service "Ollama" 11434 "/api/tags" '"models"'; then
        print_status "Ollama is already running"
        return 0
    fi
    
    print_status "Starting Ollama service..."
    sudo systemctl start ollama 2>/dev/null || {
        print_warning "Could not start Ollama via systemctl, trying direct command..."
        ollama serve &
        sleep 3
    }
    
    if check_service "Ollama" 11434 "/api/tags" '"models"'; then
        print_status "Ollama started successfully"

        while IFS= read -r configured_model; do
            [ -n "$configured_model" ] || continue
            ensure_ollama_model "$configured_model"
        done < <(configured_ollama_models)
    else
        print_error "Failed to start Ollama"
        return 1
    fi
}

# Start AI Microservice
start_microservice() {
    if ! microservice_enabled; then
        print_warning "USE_LOCAL_AI_MICROSERVICE=false, skipping AI Microservice startup"
        return 0
    fi

    print_header "Starting AI Microservice"
    
    if check_service "AI Microservice" 8000 "/health" '"status"[[:space:]]*:[[:space:]]*"healthy"'; then
        print_status "AI Microservice is already running"
        return 0
    fi

    stop_stale_microservice_listener
    
    cd "$PROJECT_ROOT/ai_microservice"
    
    # Check if virtual environment exists
    if [ ! -d "ai_microservice_env" ]; then
        print_status "Setting up Python environment..."
        ./setup.sh
    fi
    
    # Ensure directories exist
    mkdir -p "$PROJECT_ROOT/tmp" "$PROJECT_ROOT/log"
    
    local health_pattern='"status"[[:space:]]*:[[:space:]]*"healthy"'

    # Safe fallback: keep vision + video, disable likely crash-prone OCR/face/whisper stacks.
    local -a safe_mode_env=(
        "LOCAL_AI_ENABLE_VISION=true"
        "LOCAL_AI_ENABLE_VIDEO=true"
        "LOCAL_AI_ENABLE_FACE=false"
        "LOCAL_AI_ENABLE_OCR=false"
        "LOCAL_AI_ENABLE_WHISPER=false"
    )

    if ! is_truthy "$MICROSERVICE_FORCE_SAFE_MODE"; then
        # First attempt: default configuration.
        start_microservice_process "default mode"
        if wait_for_service_ready "AI Microservice" 8000 "/health" "$health_pattern" "$MICROSERVICE_STARTUP_TIMEOUT_SECONDS" && \
           ensure_service_stable "AI Microservice" 8000 "/health" "$health_pattern" "$MICROSERVICE_STABILITY_WINDOW_SECONDS"; then
            print_status "AI Microservice started successfully in default mode (PID: $MICROSERVICE_PID)"
            return 0
        fi

        print_warning "Default AI Microservice startup failed"
        stop_stale_microservice_listener

        if ! is_truthy "$MICROSERVICE_SAFE_MODE_FALLBACK"; then
            print_error "Safe mode fallback is disabled (MICROSERVICE_SAFE_MODE_FALLBACK=$MICROSERVICE_SAFE_MODE_FALLBACK)"
            return 1
        fi
    else
        print_warning "MICROSERVICE_FORCE_SAFE_MODE enabled; skipping default startup"
    fi

    start_microservice_process "safe mode (vision/video only)" "${safe_mode_env[@]}"
    if wait_for_service_ready "AI Microservice" 8000 "/health" "$health_pattern" "$MICROSERVICE_STARTUP_TIMEOUT_SECONDS" && \
       ensure_service_stable "AI Microservice" 8000 "/health" "$health_pattern" "$MICROSERVICE_STABILITY_WINDOW_SECONDS"; then
        print_warning "AI Microservice started in safe mode (PID: $MICROSERVICE_PID)"
        print_warning "Safe mode disabled: face, ocr, whisper"
        return 0
    fi

    print_error "Failed to start AI Microservice (default and safe mode)"
    return 1
}

# Stop services
stop_services() {
    print_header "Stopping Local AI Services"
    
    # Stop AI Microservice
    if [ -f "tmp/ai_microservice.pid" ]; then
        PID=$(cat tmp/ai_microservice.pid)
        if kill -0 $PID 2>/dev/null; then
            print_status "Stopping AI Microservice (PID: $PID)..."
            kill $PID
            rm tmp/ai_microservice.pid
        fi
    fi

    stop_stale_microservice_listener
    
    # Stop Ollama
    print_status "Stopping Ollama..."
    sudo systemctl stop ollama 2>/dev/null || pkill -f "ollama serve" || true
    
    print_status "All services stopped"
}

# Check status
status() {
    print_header "Local AI Services Status"
    
    echo ""
    print_status "Service Health Checks:"
    if microservice_enabled; then
        check_service "AI Microservice" 8000 "/health" '"status"[[:space:]]*:[[:space:]]*"healthy"'
    else
        print_warning "AI Microservice health check skipped (USE_LOCAL_AI_MICROSERVICE=false)"
    fi
    check_service "Ollama" 11434 "/api/tags" '"models"'
    
    echo ""
    print_status "Model Availability:"
    while IFS= read -r configured_model; do
        [ -n "$configured_model" ] || continue
        if ollama list 2>/dev/null | awk 'NR>1 {print $1}' | grep -Fxq "$configured_model"; then
            echo "✅ model available: $configured_model"
        else
            echo "❌ model missing: $configured_model"
        fi
    done < <(configured_ollama_models)
    
    echo ""
    print_status "Configuration:"
    echo "LOCAL_AI_SERVICE_URL: ${LOCAL_AI_SERVICE_URL:-http://localhost:8000}"
    echo "OLLAMA_URL: ${OLLAMA_URL:-http://localhost:11434}"
    echo "USE_LOCAL_AI_MICROSERVICE: ${USE_LOCAL_AI_MICROSERVICE:-true}"
    echo "OLLAMA_MODEL: ${OLLAMA_MODEL:-$OLLAMA_BASE_MODEL}"
    echo "OLLAMA_FAST_MODEL: ${OLLAMA_FAST_MODEL:-$OLLAMA_PRIMARY_MODEL}"
    echo "OLLAMA_QUALITY_MODEL: $OLLAMA_QUALITY_MODEL"
    echo "OLLAMA_COMMENT_MODEL: ${OLLAMA_COMMENT_MODEL:-$OLLAMA_PRIMARY_MODEL}"
    echo "OLLAMA_VISION_MODEL: ${OLLAMA_VISION_MODEL:-llama3.2-vision:11b}"
    echo "MICROSERVICE_FORCE_SAFE_MODE: $MICROSERVICE_FORCE_SAFE_MODE"
    echo "MICROSERVICE_SAFE_MODE_FALLBACK: $MICROSERVICE_SAFE_MODE_FALLBACK"
}

# Test services
test_services() {
    print_header "Testing Local AI Services"
    
    # Test AI Microservice
    print_status "Testing AI Microservice..."
    if ! microservice_enabled; then
        print_warning "AI Microservice test skipped (USE_LOCAL_AI_MICROSERVICE=false)"
    elif curl -sS --max-time "$HEALTHCHECK_TIMEOUT_SECONDS" "http://localhost:8000/health" | grep -q "healthy"; then
        print_status "✅ AI Microservice health check passed"
    else
        print_error "❌ AI Microservice health check failed"
    fi
    
    # Test Ollama
    print_status "Testing Ollama..."
    if ollama list > /dev/null 2>&1; then
        print_status "✅ Ollama is responding"
        
        # Test model generation
        echo "test" | ollama run "$OLLAMA_PRIMARY_MODEL" > /dev/null 2>&1 && \
            print_status "✅ Ollama model generation works" || \
            print_error "❌ Ollama model generation failed"
    else
        print_error "❌ Ollama is not responding"
    fi
}

cleanup_models() {
    print_header "Cleaning Up Unused Ollama Models"

    if ! command -v ollama >/dev/null 2>&1; then
        print_error "Ollama binary not found"
        return 1
    fi

    mapfile -t keep_models < <(configured_ollama_models)
    mapfile -t installed_models < <(ollama list 2>/dev/null | awk 'NR>1 {print $1}')

    if [ "${#installed_models[@]}" -eq 0 ]; then
        print_warning "No installed models found"
        return 0
    fi

    print_status "Keeping configured models: ${keep_models[*]}"
    local removed=0
    local kept=0
    local model
    for model in "${installed_models[@]}"; do
        if model_in_list "$model" "${keep_models[@]}"; then
            kept=$((kept + 1))
            continue
        fi

        print_status "Removing unused model: $model"
        if ollama rm "$model" >/dev/null 2>&1; then
            removed=$((removed + 1))
        else
            print_warning "Failed to remove model: $model"
        fi
    done

    print_status "Model cleanup complete. kept=$kept removed=$removed"
}

# Show logs
logs() {
    print_header "Service Logs"
    
    if [ -f "log/ai_microservice.log" ]; then
        echo "=== AI Microservice Logs ==="
        tail -n 50 log/ai_microservice.log
    else
        print_warning "AI Microservice logs not found"
    fi
    
    echo ""
    echo "=== Ollama Service Status ==="
    sudo systemctl status ollama --no-pager 2>/dev/null || print_warning "Ollama systemd service not found"
}

# Main script logic
case "${1:-help}" in
    start)
        start_ollama
        start_microservice
        print_status "All local AI services started!"
        ;;
    stop)
        stop_services
        ;;
    restart)
        stop_services
        sleep 2
        start_ollama
        start_microservice
        print_status "All local AI services restarted!"
        ;;
    status)
        status
        ;;
    test)
        test_services
        ;;
    cleanup-models)
        cleanup_models
        ;;
    logs)
        logs
        ;;
    help|*)
        echo "Local AI Services Management Script"
        echo ""
        echo "Usage: $0 {start|stop|restart|status|test|cleanup-models|logs|help}"
        echo ""
        echo "Commands:"
        echo "  start   - Start all local AI services (Ollama + AI Microservice)"
        echo "  stop    - Stop all local AI services"
        echo "  restart - Restart all local AI services"
        echo "  status  - Show status of all services"
        echo "  test    - Test service connectivity"
        echo "  cleanup-models - Remove Ollama models that are not configured for this app"
        echo "  logs    - Show service logs"
        echo "  help    - Show this help message"
        echo ""
        echo "Services:"
        echo "  - Ollama (LLM inference) on http://localhost:11434"
        echo "  - AI Microservice (Vision/OCR/Whisper) on http://localhost:8000"
        exit 1
        ;;
esac
